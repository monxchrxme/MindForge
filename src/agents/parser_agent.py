"""
Parser Agent —Å RAG –∏ –≤–µ–±-–ø–æ–∏—Å–∫–æ–º
"""

from typing import List, Dict, Any
import logging
from langchain_core.messages import HumanMessage

from .base_agent import BaseAgent
from ..langgraph.state_schema import GraphState, ConceptSchema
from ..utils.gigachat_client import create_gigachat_parser_client, create_gigachat_embeddings
from ..rag.chunker import TextChunker
from ..rag.vector_store import VectorStore
from ..search.web_search import WebSearchService

logger = logging.getLogger(__name__)


class ParserAgent(BaseAgent):
    """Parser Agent —Å RAG –∏ –≤–µ–±-–ø–æ–∏—Å–∫–æ–º –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤"""

    def __init__(
        self,
        gigachat_credentials: str,
        use_rag: bool = True,
        enable_web_search: bool = False
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Parser Agent

        Args:
            gigachat_credentials: GigaChat credentials
            use_rag: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RAG –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            enable_web_search: –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ñ–∞–∫—Ç—ã —á–µ—Ä–µ–∑ –≤–µ–±-–ø–æ–∏—Å–∫
        """
        super().__init__("ParserAgent")

        self.gigachat_credentials = gigachat_credentials
        self.use_rag = use_rag
        self.enable_web_search = enable_web_search

        # GigaChat –∫–ª–∏–µ–Ω—Ç —Å –ø–æ–¥—Å—á–µ—Ç–æ–º —Ç–æ–∫–µ–Ω–æ–≤
        self.llm = create_gigachat_parser_client()

        # RAG –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        if self.use_rag:
            self.chunker = TextChunker(chunk_size=500, chunk_overlap=100)
            self.embeddings = create_gigachat_embeddings()
            self.vector_store = VectorStore(self.embeddings)
            logger.info("‚úì RAG –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        # Web Search - –ò–°–ü–†–ê–í–õ–ï–ù–û: —É–±—Ä–∞–Ω –∞—Ä–≥—É–º–µ–Ω—Ç search_provider
        if self.enable_web_search:
            self.web_search = WebSearchService()  # –ë–ï–ó –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤!
            logger.info("‚úì WebSearch –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        logger.info(
            f"ParserAgent –≥–æ—Ç–æ–≤: "
            f"RAG={'ON' if use_rag else 'OFF'}, "
            f"WebSearch={'ON' if enable_web_search else 'OFF'}"
        )

    def process(self, state: GraphState) -> GraphState:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —á–µ—Ä–µ–∑ LangGraph

        Args:
            state: GraphState —Å lecture_text

        Returns:
            –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π GraphState —Å key_facts –∏ concepts
        """
        logger.info("="*50)
        logger.info("Parser Agent: –Ω–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã")
        logger.info("="*50)

        try:
            lecture_text = state.get("lecture_text", "")

            if not lecture_text:
                raise ValueError("lecture_text –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ state")

            logger.info(f"üìù –¢–µ–∫—Å—Ç –ª–µ–∫—Ü–∏–∏: {len(lecture_text)} —Å–∏–º–≤–æ–ª–æ–≤")

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ (—Å RAG –∏–ª–∏ –±–µ–∑)
            if self.use_rag and len(lecture_text) > 1000:
                logger.info("üîç –†–µ–∂–∏–º: RAG (—Ç–µ–∫—Å—Ç > 1000 —Å–∏–º–≤–æ–ª–æ–≤)")
                facts = self._extract_facts_with_rag(lecture_text)
            else:
                logger.info("üìÑ –†–µ–∂–∏–º: –ø—Ä—è–º–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")
                facts = self._extract_facts_direct(lecture_text)

            logger.info(f"‚úì –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(facts)} —Ñ–∞–∫—Ç–æ–≤")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ –≤–µ–±-–ø–æ–∏—Å–∫
            if self.enable_web_search and facts:
                logger.info("üåê –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ...")
                verification = self.web_search.verify_facts(facts, max_results=2)

                verified_facts = verification['verified_facts']
                logger.info(
                    f"‚úì –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: "
                    f"{len(verified_facts)}/{len(facts)} —Ñ–∞–∫—Ç–æ–≤ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω—ã"
                )

                facts = verified_facts

            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
            concepts = self._facts_to_concepts(facts)

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            state["key_facts"] = facts
            state["concepts"] = [c.dict() for c in concepts]
            state["messages"].append(f"Parser: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(facts)} —Ñ–∞–∫—Ç–æ–≤")

            logger.info(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(concepts)} –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –¥–ª—è QuizAgent")

            return state

        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ –≤ ParserAgent: {e}")
            import traceback
            traceback.print_exc()

            state["error"] = str(e)
            state["messages"].append(f"Parser: –û–®–ò–ë–ö–ê - {e}")
            return state

    def _extract_facts_with_rag(self, lecture_text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG"""
        logger.info("üîç RAG: –®–∞–≥ 1/4 - –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏")
        chunks = self.chunker.split(lecture_text)
        logger.info(f"   –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")

        logger.info("üîç RAG: –®–∞–≥ 2/4 - –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ FAISS")
        self.vector_store.create_from_texts(chunks)

        logger.info("üîç RAG: –®–∞–≥ 3/4 - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        query = "–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, —Ñ–æ—Ä–º—É–ª—ã –∏ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã"
        relevant_docs = self.vector_store.similarity_search(query, k=min(5, len(chunks)))

        logger.info(f"   –ù–∞–π–¥–µ–Ω–æ {len(relevant_docs)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –≤ –µ–¥–∏–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        logger.info(f"   –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤")

        logger.info("üîç RAG: –®–∞–≥ 4/4 - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ LLM")
        facts = self._extract_facts_from_context(context)

        logger.info(f"‚úì RAG –∑–∞–≤–µ—Ä—à—ë–Ω: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(facts)} —Ñ–∞–∫—Ç–æ–≤")
        return facts

    def _extract_facts_direct(self, lecture_text: str) -> List[str]:
        """–ü—Ä—è–º–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ –±–µ–∑ RAG"""
        logger.info("–ü—Ä—è–º–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–≤—ã—Ö 3000 —Å–∏–º–≤–æ–ª–æ–≤")
        return self._extract_facts_from_context(lecture_text[:3000])

    def _extract_facts_from_context(self, context: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LLM"""
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –ª–µ–∫—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ–∫–∏ 5-10 –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤.

–¢–µ–∫—Å—Ç:
{context}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
1. –ö–∞–∂–¥—ã–π —Ñ–∞–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º
2. –ß–µ—Ç–∫–∞—è –∏ –∫—Ä–∞—Ç–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞
3. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, —Ñ–æ—Ä–º—É–ª—ã, –∫–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏
4. –ë–ï–ó LaTeX —Ñ–æ—Ä–º—É–ª - –∏—Å–ø–æ–ª—å–∑—É–π –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç

–§–æ—Ä–º–∞—Ç: —Å–ø–∏—Å–æ–∫, –∫–∞–∂–¥—ã–π —Ñ–∞–∫—Ç —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "- "
"""

        message = HumanMessage(content=prompt)
        response = self.llm.chat(message)

        # –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–∫—Ç–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞
        facts = []
        for line in response.content.split('\n'):
            line = line.strip()
            if line.startswith('-') or line.startswith('‚Ä¢') or line.startswith('*'):
                fact = line.lstrip('-‚Ä¢* ').strip()
                if fact and len(fact) > 10:
                    facts.append(fact)

        # –ï—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –±–µ—Ä—ë–º –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        if not facts:
            facts = [
                line.strip()
                for line in response.content.split('\n')
                if line.strip() and len(line.strip()) > 10
            ][:10]

        logger.info(f"   –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(facts)} —Ñ–∞–∫—Ç–æ–≤")
        return facts[:10]

    def _facts_to_concepts(self, facts: List[str]) -> List[ConceptSchema]:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–∞–∫—Ç–æ–≤ –≤ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –¥–ª—è QuizAgent

        Args:
            facts: —Å–ø–∏—Å–æ–∫ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤

        Returns:
            —Å–ø–∏—Å–æ–∫ ConceptSchema –æ–±—ä–µ–∫—Ç–æ–≤
        """
        concepts = []

        for i, fact in enumerate(facts):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å: –ø–µ—Ä–≤—ã–µ 3 - high, 4-6 - medium, –æ—Å—Ç–∞–ª—å–Ω—ã–µ - low
            if i < 3:
                importance = "high"
            elif i < 6:
                importance = "medium"
            else:
                importance = "low"

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–¥–æ –ø–µ—Ä–≤–æ–π —Ç–æ—á–∫–∏ –∏–ª–∏ –ø–µ—Ä–≤—ã–µ 50 —Å–∏–º–≤–æ–ª–æ–≤)
            title = fact.split('.')[0][:50] if '.' in fact else fact[:50]

            concept = ConceptSchema(
                title=title,
                description=fact,
                importance=importance,
                context=fact
            )
            concepts.append(concept)

        return concepts
